{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Part 3 solution\n",
    "\n",
    "This notebook focuses only on the learning-to-rank (LTR) part of the assignment. It takes pre-computed feature vectors for query-document pairs as input. Importantly, query-document pairs are sorted by the BM25 score (feature #0).\n",
    "\n",
    "You should be able to run this notebook as is. If you upload the resulting `ltr2.txt` output file to Kaggle, it'll give you an NDCG@20 score of 0.10044."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading features file**\n",
    "\n",
    "  - `ignore` contains a list of feature IDs that are ignored when loading\n",
    "  - `max_docs` is the maximum number of (unlabeled) docs loaded per query (mind that you should sort instances beforehand if you want to use this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_features(features_file, ignore=[], max_docs=10000):\n",
    "    X, y, qids, doc_ids = [], [], [], []\n",
    "    with open(features_file, \"r\") as f:\n",
    "        i, s_qid = 0, None\n",
    "        n = 0  # total number of docs for the current query\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            label = 0 if items[0] == \"?\" else int(items[0])\n",
    "            qid = items[1]\n",
    "            doc_id = items[2]\n",
    "            tmp = np.array([float(i.split(\":\")[1]) for i in items[3:]])\n",
    "            features = [tmp[j] for j in range(len(tmp)) if j not in ignore]\n",
    "            \n",
    "            if s_qid != qid:  # new query seen\n",
    "                s_qid = qid\n",
    "                n = 0\n",
    "            n += 1\n",
    "            \n",
    "            if (n <= max_docs) or (items[0] != \"?\"):  # max number of documents \n",
    "                X.append(features)\n",
    "                y.append(label)\n",
    "                qids.append(qid)\n",
    "                doc_ids.append(doc_id)\n",
    "\n",
    "    return X, y, qids, doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-max feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minmax_norm(features, qids):\n",
    "    \"\"\"Normalizes all features.\"\"\"\n",
    "    n = len(features[0]) # number of features\n",
    "    for qid in set(qids):\n",
    "        min_x = [10000] * n # sufficiently large number\n",
    "        max_x = [-10000] * n # # sufficiently small number                \n",
    "\n",
    "        # finding min and max values        \n",
    "        for i in range(len(features)):\n",
    "            if qids[i] == qid:\n",
    "                for j in range(n):\n",
    "                    x = features[i][j]\n",
    "                    if x != -1:\n",
    "                        if x < min_x[j]:\n",
    "                            min_x[j] = x\n",
    "                        if x > max_x[j]:\n",
    "                            max_x[j] = x\n",
    "\n",
    "        # applying normalization\n",
    "        for i in range(len(features)):\n",
    "            if qids[i] == qid:\n",
    "                for j in range(n):\n",
    "                    if max_x[j] == min_x[j]:  # no norm for that feature\n",
    "                        continue\n",
    "                    x = features[i][j]\n",
    "                    if x != -1:\n",
    "                        features[i][j] = (x - min_x[j]) / (max_x[j] - min_x[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise LTR class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PointWiseLTRModel(object):\n",
    "    def __init__(self, regressor):\n",
    "        \"\"\"\n",
    "        :param classifier: an instance of scikit-learn regressor\n",
    "        \"\"\"\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def _train(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains and LTR model.\n",
    "        :param X: features of training instances\n",
    "        :param y: relevance assessments of training instances\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.regressor is not None\n",
    "        self.model = self.regressor.fit(X, y)\n",
    "\n",
    "        \n",
    "    def rank(self, ft, doc_ids):\n",
    "        \"\"\"\n",
    "        Predicts relevance labels and rank documents for a given query\n",
    "        :param ft: a list of features for query-doc pairs\n",
    "        :param ft: a list of document ids\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        rel_labels = self.model.predict(ft)\n",
    "        results = []\n",
    "        for i in range(len(doc_ids)):\n",
    "            results.append((doc_ids[i], rel_labels[i]))\n",
    "        \n",
    "        return sorted(results, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `features.txt` file contains the top-200 documents retrieved using BM25 on the content field. The first number in this file is the relevance label according to the ground truth (`qrels.csv`) if the query-document pair is present there; otherwise, the value is \"?\". We will treat documents with missing relevance labels as non-relevant (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATURES_FILE = \"data/features_sorted.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATURES = [\"BM25 content score\", \"BM25 title score\", \"BM25 anchors score\", \"LM content score\", \n",
    "            \"LM title score\", \"LM anchors score\", \"query length\", \"sum query term IDF in content\", \n",
    "            \"avg query term IDF in content\", \"sum query term IDF in title\", \n",
    "            \"avg query term IDF in title\", \"sum query term IDF in anchors\", \n",
    "            \"avg query term IDF in anchors\", \"PageRank\", \"content field length\", \n",
    "            \"title field length\", \"anchors field length\", \"content sum TFIDF\", \"title sum TFIDF\", \n",
    "            \"anchors sum TFIDF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, train_y, train_qids, train_doc_ids = load_features(FEATURES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minmax_norm(train_X, train_qids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=0)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr._train(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output feature importance according to the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 title score              : 0.22798805739892966\n",
      "BM25 content score            : 0.13885445479937622\n",
      "content sum TFIDF             : 0.12117450414966097\n",
      "LM title score                : 0.1135795023442336\n",
      "sum query term IDF in title   : 0.09042303921443162\n",
      "content field length          : 0.0687523840711894\n",
      "BM25 anchors score            : 0.0533051958591014\n",
      "sum query term IDF in anchors : 0.05208598514389976\n",
      "LM content score              : 0.032096002884159223\n",
      "avg query term IDF in anchors : 0.02443032849236756\n",
      "avg query term IDF in content : 0.02331145185636168\n",
      "anchors sum TFIDF             : 0.013677520504176012\n",
      "sum query term IDF in content : 0.009995155621257669\n",
      "title field length            : 0.008774318029963708\n",
      "title sum TFIDF               : 0.0069109412495633234\n",
      "LM anchors score              : 0.006778708992783746\n",
      "anchors field length          : 0.003698505015740769\n",
      "PageRank                      : 0.0036012033532626707\n",
      "query length                  : 0.0005627410195409994\n",
      "avg query term IDF in title   : 0.0\n"
     ]
    }
   ],
   "source": [
    "imp = zip(FEATURES, clf.feature_importances_)\n",
    "imp_sorted = sorted(imp, key=lambda imps: imps[1], reverse=True)\n",
    "print(\"\\n\".join([\"{:30}: {}\".format(k,v) for k, v in imp_sorted]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** One key thing to notice here is that the BM25 title score is considered the most important feature. Using a single-field BM25 baseline on the training query set, the NDCG@20 scores on the title and content fields are 0.1106 and 0.1290, respectively. On the test query set, however, these scores, according to Kaggle, are 0.09268 and 0.03336, respectively. Notice that the title field works much worse on the test query set. (That is why both the title and content baselines were made available on Kaggle, so that you can spot this...) Thus, the learned model that assigns very high importance to the title field is not expected to work very well on the test data. Therefore, we drop the \"BM25 title\" (#1) and \"LM title\" (#4) features from our feature set.\n",
    "\n",
    "After removing these two title features, we find that \"BM25 anchors\" (#2) ends up as the top feature. We find -experimentally- that removing that one further helps a bit. This suggests that the training and test data have slightly different characteristics, and we'd otherwise be overfitting on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the features again (excepting features #1, #2, #4) and train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_drop = [1, 2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in sorted(features_drop, reverse=True):\n",
    "    FEATURES.pop(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_y, train_qids, train_doc_ids = load_features(FEATURES_FILE, features_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minmax_norm(train_X, train_qids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=0)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr._train(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importances for our updated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 content score            : 0.2660943723057572\n",
      "LM anchors score              : 0.22969573699217333\n",
      "sum query term IDF in anchors : 0.1453840894831454\n",
      "content field length          : 0.10866177320580166\n",
      "sum query term IDF in title   : 0.10798806527648358\n",
      "content sum TFIDF             : 0.07006541937063403\n",
      "LM content score              : 0.017759206525096313\n",
      "avg query term IDF in content : 0.01378558972528977\n",
      "sum query term IDF in content : 0.012020659268348596\n",
      "PageRank                      : 0.010507015533364005\n",
      "anchors field length          : 0.008601025562756796\n",
      "anchors sum TFIDF             : 0.0045592198415815265\n",
      "avg query term IDF in anchors : 0.00232070100566144\n",
      "title sum TFIDF               : 0.0018263705085941981\n",
      "avg query term IDF in title   : 0.0006495009408379531\n",
      "query length                  : 8.12544544743667e-05\n",
      "title field length            : 0.0\n"
     ]
    }
   ],
   "source": [
    "imp = zip(FEATURES, clf.feature_importances_)\n",
    "imp_sorted = sorted(imp, key=lambda imps: imps[1], reverse=True)\n",
    "print(\"\\n\".join([\"{:30}: {}\".format(k,v) for k, v in imp_sorted]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the model on unseen queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY2_FILE = \"data/queries2.txt\"\n",
    "FEATURES2_FILE = \"data/features2_sorted.txt\"\n",
    "OUTPUT_FILE = \"data/ltr2.txt\"\n",
    "TOP_DOCS = 20  # this many top docs to write to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries = load_queries(QUERY2_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** Our final trick is here. Instead of reranking the top-200 documents for a new, unseen query, we only rerank the top-50. We trust the baseline BM25 ranker and play a \"bit safer\". (This makes a difference about 0.007 NDCG@20 on Kaggle -- just that extra final bit that you needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, _, qids, doc_ids = load_features(FEATURES2_FILE, features_drop, max_docs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minmax_norm(X, qids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply model and write results to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking query #251 'identifying spider bites'\n",
      "Ranking query #252 'history of orcas island'\n",
      "Ranking query #253 'tooth abscess'\n",
      "Ranking query #254 'barrett's esophagus'\n",
      "Ranking query #255 'teddy bears'\n",
      "Ranking query #256 'patron saint of mental illness'\n",
      "Ranking query #257 'holes by louis sachar'\n",
      "Ranking query #258 'hip roof'\n",
      "Ranking query #259 'carpenter bee'\n",
      "Ranking query #260 'the american revolutionary'\n",
      "Ranking query #261 'folk remedies sore throat'\n",
      "Ranking query #262 'balding cure'\n",
      "Ranking query #263 'evidence for evolution'\n",
      "Ranking query #264 'tribe formerly living in alabama'\n",
      "Ranking query #265 'F5 tornado'\n",
      "Ranking query #266 'symptoms of heart attack'\n",
      "Ranking query #267 'feliz navidad lyrics'\n",
      "Ranking query #268 'benefits of running'\n",
      "Ranking query #269 'marshall county schools'\n",
      "Ranking query #270 'sun tzu'\n",
      "Ranking query #271 'halloween activities for middle school'\n",
      "Ranking query #272 'dreams interpretation'\n",
      "Ranking query #273 'wilson's disease'\n",
      "Ranking query #274 'golf instruction'\n",
      "Ranking query #275 'uss cole'\n",
      "Ranking query #276 'how has african american music influence history'\n",
      "Ranking query #277 'bewitched cast'\n",
      "Ranking query #278 'mister rogers'\n",
      "Ranking query #279 'game theory'\n",
      "Ranking query #280 'view my internet history'\n",
      "Ranking query #281 'ketogenic diet'\n",
      "Ranking query #282 'nasa interplanetary missions'\n",
      "Ranking query #283 'hayrides in pa'\n",
      "Ranking query #284 'where to find morel mushrooms'\n",
      "Ranking query #285 'magnesium rich foods'\n",
      "Ranking query #286 'common schizophrenia drugs'\n",
      "Ranking query #287 'carotid cavernous fistula treatment'\n",
      "Ranking query #288 'fidel castro'\n",
      "Ranking query #289 'benefits of yoga'\n",
      "Ranking query #290 'norway spruce'\n",
      "Ranking query #291 'sangre de cristo mountains'\n",
      "Ranking query #292 'history of the electronic medical record'\n",
      "Ranking query #293 'educational advantages of social networking sites'\n",
      "Ranking query #294 'flowering plants'\n",
      "Ranking query #295 'how to tie a windsor knot'\n",
      "Ranking query #296 'recycling lead acid batteries'\n",
      "Ranking query #297 'altitude sickness'\n",
      "Ranking query #298 'medical care and jehovah's witnesses'\n",
      "Ranking query #299 'pink slime in ground beef'\n",
      "Ranking query #300 'how to find the mean'\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_FILE, \"w\") as fout:\n",
    "    \n",
    "    fout.write(\"QueryId,DocumentId\\n\")\n",
    "    \n",
    "    for qid, query in sorted(queries.items()):\n",
    "        print(\"Ranking query #{} '{}'\".format(qid, query))\n",
    "\n",
    "        fts = []\n",
    "        ids = []\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            if qids[i] == qid:\n",
    "                fts.append(X[i])\n",
    "                ids.append(doc_ids[i])\n",
    "                \n",
    "        # Get ranking\n",
    "        r = ltr.rank(fts, ids)    \n",
    "        # Write the results to file\n",
    "        rank = 1\n",
    "        for doc_id, score in r:\n",
    "            if rank <= TOP_DOCS:\n",
    "                fout.write(qid + \",\" + doc_id + \"\\n\")                            \n",
    "            rank += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "That's all folks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
